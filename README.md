# rag-chatbot-langchain
rag-chatbot-langchain
RAG Chatbot using LangChain, Gradio & Local VectorStore

A fully local Retrieval-Augmented Generation (RAG) chatbot built using LangChain, Gradio, and a persistent Chroma vectorstore.
This project allows you to ingest documents, generate embeddings, index them into a vector database, and chat with your private data securely.

ğŸš€ Features

ğŸ” Document Ingestion Pipeline (ingest.py)

ğŸ§  Embeddings + Vectorstore (Chroma)

ğŸ¤– LangChain RetrievalQA / RAG pipeline

ğŸ’¬ Interactive Gradio Chat UI

ğŸ“ Local persistent vector store (no cloud dependency)

âš¡ Fast query response using semantic search

ğŸ” Supports private document Q&A securely

ğŸ“‚ Project Structure
rag-chatbot-langchain/
â”‚â”€â”€ data/               # Your PDFs / text files go here
â”‚â”€â”€ vectorstore/        # Persistent Chroma DB
â”‚â”€â”€ ingest.py           # Script to ingest data
â”‚â”€â”€ rag_app.py          # Gradio RAG chatbot
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .env
â”‚â”€â”€ README.md

ğŸ§© Architecture
End-to-End Flow

Document Ingestion

Loads PDFs/text files from /data

Splits text into chunks

Creates embeddings (OpenAI / OCI / HuggingFace)

Stores vectors in ChromaDB

Chat Query

User sends question via Gradio

Query gets embedded

Vectorstore retrieves relevant chunks

RAG Answer

Retrieved context + question â†’ LLM

LLM produces grounded, accurate response

Response streamed back to Gradio
